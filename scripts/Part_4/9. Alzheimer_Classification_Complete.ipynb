{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1fd9fdf",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Project Requirements Checklist\n",
    "\n",
    "âœ… **4.1 Task Selection**: Classification task justified by categorical labels  \n",
    "âœ… **4.2 Data Preparation**: Train/test split with preprocessing  \n",
    "âœ… **4.3 Architecture & Training**: ResNet50 with training diagnosis  \n",
    "âœ… **4.4 Model Evaluation**: Comprehensive metrics on test set  \n",
    "\n",
    "**Dataset**: Alzheimer's 4-class MRI  \n",
    "**Task**: Multi-class Image Classification  \n",
    "**Model**: ResNet50 (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40dcb424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Device: cpu\n",
      "âœ“ Results: c:\\Users\\ottav\\OneDrive - Politecnico di Milano\\Desktop\\ComplexData\\AlzheimerComplexDataProject/Results/MRI_Classification\n"
     ]
    }
   ],
   "source": [
    "# 1. SETUP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ Device: {device}\")\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(\"../../..\")  # Go up to FinalProject root from scripts/Part_4/\n",
    "DATA_DIR = PROJECT_ROOT / \"Data\" / \"Alzheimer_MRI\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"Results\" / \"MRI_Classification\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"âœ“ Project root: {PROJECT_ROOT.absolute()}\")\n",
    "print(f\"âœ“ Data dir: {DATA_DIR.absolute()}\")\n",
    "print(f\"âœ“ Results: {RESULTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e1557",
   "metadata": {},
   "source": [
    "## **Requirement 4.1: Task Selection**\n",
    "\n",
    "Download from: https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images\n",
    "\n",
    "Expected structure:\n",
    "```\n",
    "Data/Alzheimer_MRI/\n",
    "â”œâ”€â”€ train/\n",
    "â”‚   â”œâ”€â”€ MildDemented/\n",
    "â”‚   â”œâ”€â”€ ModerateDemented/\n",
    "â”‚   â”œâ”€â”€ NonDemented/\n",
    "â”‚   â””â”€â”€ VeryMildDemented/\n",
    "â””â”€â”€ test/\n",
    "    â””â”€â”€ [same structure]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec882f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Dataset not found! Please download first.\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset\n",
    "classes = [\"Mild Dementia\", \"Moderate Dementia\", \"Non Demented\", \"Very mild Dementia\"]\n",
    "class_paths = [DATA_DIR / cls for cls in classes]\n",
    "\n",
    "if all(path.exists() for path in class_paths):\n",
    "    print(\"=\"*70)\n",
    "    print(\"TASK SELECTION ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Dataset Type: Labeled image folders\")\n",
    "    print(f\"Classes: {classes}\")\n",
    "    print(f\"Number: {len(classes)}\")\n",
    "    print(f\"\\nðŸŽ¯ TASK: Multi-class Classification\")\n",
    "    print(f\"Rationale: Categorical labels â†’ Classification task\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"âŒ Dataset not found! Expected folders:\")\n",
    "    for cls in classes:\n",
    "        print(f\"  - {DATA_DIR / cls}\")\n",
    "    print(\"\\nPlease ensure the dataset is properly organized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da72f0e",
   "metadata": {},
   "source": [
    "## **Requirement 4.2: Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40376c1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\ottav\\\\OneDrive - Politecnico di Milano\\\\Desktop\\\\ComplexData\\\\AlzheimerComplexDataProject/Data/Alzheimer_MRI/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m             stats[class_dir.name] = \u001b[38;5;28mlen\u001b[39m(imgs)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stats\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_stats = \u001b[43mcount_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m test_stats = count_images(test_dir)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“ˆ DATASET STATISTICS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mcount_images\u001b[39m\u001b[34m(directory)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount_images\u001b[39m(directory):\n\u001b[32m      3\u001b[39m     stats = {}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m class_dir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdirectory\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m class_dir.is_dir():\n\u001b[32m      6\u001b[39m             imgs = \u001b[38;5;28mlist\u001b[39m(class_dir.glob(\u001b[33m'\u001b[39m\u001b[33m*.jpg\u001b[39m\u001b[33m'\u001b[39m)) + \u001b[38;5;28mlist\u001b[39m(class_dir.glob(\u001b[33m'\u001b[39m\u001b[33m*.png\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py:575\u001b[39m, in \u001b[36mPath.iterdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Yield path objects of the directory contents.\u001b[39;00m\n\u001b[32m    570\u001b[39m \n\u001b[32m    571\u001b[39m \u001b[33;03mThe children are yielded in arbitrary order, and the\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[33;03mspecial entries '.' and '..' are not included.\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    574\u001b[39m root_dir = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m scandir_it:\n\u001b[32m    576\u001b[39m     paths = [entry.path \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m scandir_it]\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m root_dir == \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\ottav\\\\OneDrive - Politecnico di Milano\\\\Desktop\\\\ComplexData\\\\AlzheimerComplexDataProject/Data/Alzheimer_MRI/train'"
     ]
    }
   ],
   "source": [
    "# Load dataset and create train/test split\n",
    "# Load all images and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for class_label, class_name in enumerate(classes):\n",
    "    class_dir = DATA_DIR / class_name\n",
    "    files = list(class_dir.glob('*.jpg'))\n",
    "    print(f\"Class: {class_name}, Files Found: {len(files)}\")\n",
    "    for file_path in files:\n",
    "        image_paths.append(str(file_path))\n",
    "        labels.append(class_label)\n",
    "\n",
    "if len(image_paths) == 0:\n",
    "    raise ValueError(\"No images found. Check dataset folder names or file paths.\")\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    image_paths, labels, test_size=0.15, shuffle=True, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal images: {len(image_paths)}\")\n",
    "print(f\"Train: {len(x_train)} ({100*len(x_train)/len(image_paths):.1f}%)\")\n",
    "print(f\"Test: {len(x_test)} ({100*len(x_test)/len(image_paths):.1f}%)\")\n",
    "\n",
    "# Dataset statistics\n",
    "def count_class_distribution(label_list, classes):\n",
    "    stats = {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        count = label_list.count(i)\n",
    "        stats[cls] = count\n",
    "    return stats\n",
    "\n",
    "train_stats = count_class_distribution(y_train, classes)\n",
    "test_stats = count_class_distribution(y_test, classes)\n",
    "\n",
    "print(\"\\nðŸ“ˆ DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Class':<25} {'Train':<10} {'Test':<10} {'Total':<10}\")\n",
    "print(\"-\"*70)\n",
    "for cls in sorted(train_stats.keys()):\n",
    "    tr, te = train_stats[cls], test_stats[cls]\n",
    "    print(f\"{cls:<25} {tr:<10} {te:<10} {tr+te:<10}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.bar(train_stats.keys(), train_stats.values(), color='steelblue')\n",
    "ax1.set_title('Training Set', fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax2.bar(test_stats.keys(), test_stats.values(), color='coral')\n",
    "ax2.set_title('Test Set', fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '01_distribution.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46839e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing with noise reduction\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.GaussianBlur(3, sigma=(0.1, 2.0)),  # Noise reduction\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"âœ“ Preprocessing defined:\")\n",
    "print(\"  â€¢ Noise reduction: Gaussian blur\")\n",
    "print(\"  â€¢ Augmentation: Flip, rotate, color jitter\")\n",
    "print(\"  â€¢ Normalization: ImageNet statistics\")\n",
    "\n",
    "# Custom Dataset class for our data structure\n",
    "class AlzheimerDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = datasets.folder.default_loader(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a331335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "# Split train into train + validation (80-20 of training data)\n",
    "train_val_paths, val_paths, train_val_labels, val_labels = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, shuffle=True, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "train_dataset = AlzheimerDataset(train_val_paths, train_val_labels, transform=data_transforms['train'])\n",
    "val_dataset = AlzheimerDataset(val_paths, val_labels, transform=data_transforms['test'])\n",
    "test_dataset = AlzheimerDataset(x_test, y_test, transform=data_transforms['test'])\n",
    "\n",
    "class_names = classes\n",
    "print(f\"\\nâœ“ Splits: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "print(f\"âœ“ Classes: {class_names}\")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13015f85",
   "metadata": {},
   "source": [
    "## **Requirement 4.3: Architecture Selection & Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class AlzheimerClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Freeze backbone\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Custom classifier\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "model = AlzheimerClassifier(len(class_names)).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Base: ResNet50 (ImageNet pretrained)\")\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "print(f\"Trainable: {trainable:,}\")\n",
    "print(f\"Frozen: {total_params-trainable:,}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991253d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "num_epochs = 25\n",
    "early_stop_patience = 7\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"âœ“ Configuration:\")\n",
    "print(f\"  â€¢ Epochs: {num_epochs}\")\n",
    "print(f\"  â€¢ Optimizer: Adam (lr=0.001, wd=1e-4)\")\n",
    "print(f\"  â€¢ Scheduler: ReduceLROnPlateau\")\n",
    "print(f\"  â€¢ Early stopping: {early_stop_patience} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce197c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc='Train', leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / total, 100. * correct / total\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Val', leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return running_loss / total, 100. * correct / total, all_preds, all_labels\n",
    "\n",
    "print(\"âœ“ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf63e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"LR: {current_lr:.2e}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), RESULTS_DIR / 'best_model.pth')\n",
    "        print(f\"âœ“ Best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"\\nâš  Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Best Val Acc: {best_val_acc:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bbfded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training diagnostics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(history['train_loss'], 'o-', label='Train')\n",
    "axes[0, 0].plot(history['val_loss'], 's-', label='Val')\n",
    "axes[0, 0].set_title('Loss', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history['train_acc'], 'o-', label='Train')\n",
    "axes[0, 1].plot(history['val_acc'], 's-', label='Val')\n",
    "axes[0, 1].set_title('Accuracy', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(history['lr'], 'o-', color='red')\n",
    "axes[1, 0].set_title('Learning Rate', fontweight='bold')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "diff = np.array(history['train_acc']) - np.array(history['val_acc'])\n",
    "axes[1, 1].plot(diff, 'o-', color='purple')\n",
    "axes[1, 1].axhline(0, color='k', linestyle='--')\n",
    "axes[1, 1].axhline(5, color='r', linestyle='--', label='5% threshold')\n",
    "axes[1, 1].set_title('Overfitting Analysis (Train-Val)', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Diagnostics', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '02_training.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Diagnosis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING DIAGNOSIS\")\n",
    "print(\"=\"*70)\n",
    "final_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "if final_gap > 10:\n",
    "    print(\"âš  Overfitting detected (gap > 10%)\")\n",
    "elif final_gap > 5:\n",
    "    print(\"âš  Mild overfitting (gap 5-10%)\")\n",
    "else:\n",
    "    print(\"âœ“ Well-generalized (gap < 5%)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e55a73",
   "metadata": {},
   "source": [
    "## **Requirement 4.4: Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load(RESULTS_DIR / 'best_model.pth'))\n",
    "test_loss, test_acc, test_preds, test_labels = validate(model, test_loader, criterion)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbffa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nCLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d007fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True', fontweight='bold')\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '03_confusion_matrix.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    acc = 100 * cm[i, i] / cm[i].sum()\n",
    "    print(f\"  {name:<25}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "model.eval()\n",
    "all_probs, all_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, 1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_true.append(labels.numpy())\n",
    "\n",
    "all_probs = np.vstack(all_probs)\n",
    "all_true = np.concatenate(all_true)\n",
    "\n",
    "y_bin = label_binarize(all_true, classes=range(len(class_names)))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "auc_scores = []\n",
    "\n",
    "for i, name in enumerate(class_names):\n",
    "    fpr, tpr, _ = roc_curve(y_bin[:, i], all_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    auc_scores.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC={roc_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "plt.title('ROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / '04_roc.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMean AUC: {np.mean(auc_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f51832",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f31cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'Task': 'Multi-class Classification',\n",
    "    'Classes': len(class_names),\n",
    "    'Architecture': 'ResNet50 (Transfer Learning)',\n",
    "    'Test Accuracy': f'{test_acc:.2f}%',\n",
    "    'Mean AUC': f'{np.mean(auc_scores):.4f}',\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k:<20}: {v}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… PROJECT REQUIREMENTS COMPLETED:\")\n",
    "print(\"  4.1 Task Selection âœ“\")\n",
    "print(\"  4.2 Data Preparation âœ“\")\n",
    "print(\"  4.3 Architecture & Training âœ“\")\n",
    "print(\"  4.4 Model Evaluation âœ“\")\n",
    "\n",
    "print(f\"\\nðŸ“ Results saved to: {RESULTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
